"""
æ–‡æ¡£å¤„ç†æœåŠ¡ - æ ¸å¿ƒå¤„ç†æµç¨‹
"""
from pathlib import Path
from typing import Dict, Any, List
from sqlalchemy.orm import Session
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from uuid import UUID
import asyncio

from app.config import settings
from app.models.document import Document as DBDocument, DocumentChunk
from app.services.parsers import parser_factory
from app.services.milvus_store import milvus_store
from app.services.hybrid_retriever import hybrid_retriever


class DocumentProcessorService:
    """æ–‡æ¡£å¤„ç†æœåŠ¡"""

    def __init__(self):
        # LangChain æ–‡æœ¬åˆ‡ç‰‡å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""],
            length_function=len,
        )

    async def process_document(
        self,
        file_path: Path,
        document_id: UUID,
        db: Session
    ) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹

        æµç¨‹ï¼š
        1. è§£ææ–‡æ¡£
        2. æ–‡æœ¬åˆ‡ç‰‡
        3. ç”Ÿæˆ Embeddings
        4. å­˜å…¥å‘é‡åº“
        5. ä¿å­˜åˆ°æ•°æ®åº“

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£ ID
            db: æ•°æ®åº“ä¼šè¯

        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # Step 1: æ›´æ–°çŠ¶æ€ä¸º processing
            await self._update_status(
                db, document_id, "processing", 0, "parsing"
            )

            # Step 2: è§£ææ–‡æ¡£
            print(f"Parsing document: {file_path}")
            documents = parser_factory.parse(file_path)

            await self._update_status(
                db, document_id, "processing", 33, "chunking"
            )

            # Step 3: æ–‡æœ¬åˆ‡ç‰‡
            print(f"Chunking document into smaller pieces...")
            chunks = self.text_splitter.split_documents(documents)

            # ä¸ºæ¯ä¸ª chunk æ·»åŠ å…ƒæ•°æ®
            for idx, chunk in enumerate(chunks):
                chunk.metadata['document_id'] = str(document_id)
                chunk.metadata['chunk_index'] = idx

            await self._update_status(
                db, document_id, "processing", 66, "embedding"
            )

            # Step 4: ç”Ÿæˆ Embeddings å¹¶å­˜å…¥ Milvus
            print(f"Generating embeddings and storing in Milvus...")

            # è½¬æ¢ä¸º Milvus éœ€è¦çš„æ ¼å¼
            milvus_docs = []
            for chunk in chunks:
                milvus_docs.append({
                    'content': chunk.page_content,
                    'metadata': chunk.metadata
                })

            vector_ids = milvus_store.add_documents(
                milvus_docs, document_id
            )

            # Step 5: ä¿å­˜åˆ‡ç‰‡åˆ°æ•°æ®åº“
            print(f"Saving chunks to PostgreSQL...")
            await self._save_chunks_to_db(
                db, document_id, chunks, vector_ids
            )

            # Step 6: æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºå®Œæˆ
            total_chars = sum(len(c.page_content) for c in chunks)
            await self._update_status(
                db,
                document_id,
                "completed",
                100,
                "completed",
                chunk_count=len(chunks),
                total_characters=total_chars
            )

            print(f"âœ… Document processed successfully: {len(chunks)} chunks")

            # Step 7: é‡æ–°æ„å»º BM25 ç´¢å¼•ï¼ˆåŒ…å«æ‰€æœ‰æ–‡æ¡£ï¼‰
            await self._rebuild_bm25_index(db)

            return {
                "status": "success",
                "chunk_count": len(chunks),
                "total_characters": total_chars
            }

        except Exception as e:
            # é”™è¯¯å¤„ç†
            print(f"âŒ Error processing document: {str(e)}")
            await self._update_status(
                db,
                document_id,
                "failed",
                0,
                "failed",
                error_message=str(e)
            )
            raise

    async def _update_status(
        self,
        db: Session,
        document_id: UUID,
        status: str,
        progress: int,
        stage: str,
        **kwargs
    ):
        """æ›´æ–°æ–‡æ¡£å¤„ç†çŠ¶æ€"""
        db_doc = db.query(DBDocument).filter(
            DBDocument.id == document_id
        ).first()

        if db_doc:
            db_doc.status = status
            db_doc.processing_progress = progress
            db_doc.current_stage = stage

            for key, value in kwargs.items():
                setattr(db_doc, key, value)

            db.commit()
            db.refresh(db_doc)

    async def _save_chunks_to_db(
        self,
        db: Session,
        document_id: UUID,
        chunks: List[Document],
        vector_ids: List[str]
    ):
        """ä¿å­˜åˆ‡ç‰‡åˆ°æ•°æ®åº“"""
        for idx, (chunk, vector_id) in enumerate(zip(chunks, vector_ids)):
            db_chunk = DocumentChunk(
                document_id=document_id,
                chunk_index=idx,
                content=chunk.page_content,
                page_number=chunk.metadata.get('page'),
                metadata=chunk.metadata,
                vector_id=vector_id
            )
            db.add(db_chunk)

        db.commit()

    async def _rebuild_bm25_index(self, db: Session):
        """é‡æ–°æ„å»º BM25 ç´¢å¼•ï¼ˆåŒ…å«æ‰€æœ‰å·²å®Œæˆçš„æ–‡æ¡£ç‰‡æ®µï¼‰"""
        try:
            # æŸ¥è¯¢æ‰€æœ‰å·²å®Œæˆæ–‡æ¡£çš„ç‰‡æ®µ
            all_chunks = db.query(DocumentChunk).join(DBDocument).filter(
                DBDocument.status == 'completed'
            ).all()

            if all_chunks:
                print(f"ğŸ”„ Rebuilding BM25 index with {len(all_chunks)} chunks...")
                hybrid_retriever.build_bm25_index(all_chunks)
            else:
                print("âš ï¸  No chunks found for BM25 index")

        except Exception as e:
            print(f"âš ï¸  Failed to rebuild BM25 index: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“æ–‡æ¡£å¤„ç†æµç¨‹


# å…¨å±€å®ä¾‹
document_processor = DocumentProcessorService()
